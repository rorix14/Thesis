#define MATRIX_THREAD_COUNT_X 8
#define MATRIX_THREAD_COUNT_Y 8

#pragma kernel forward_pass_ReLU
#pragma kernel backwards_pass_ReLU_inputs
#pragma kernel backwards_pass_ReLU_weights_biases_Adam

#pragma kernel forward_pass_Tanh
#pragma kernel backwards_pass_Tanh_inputs
#pragma kernel backwards_pass_Tanh_weights_biases_Adam

#pragma kernel forward_pass_softmax
//#pragma kernel forward_pass_softmax_normalize
#pragma kernel backwards_pass_softmax_inputs
#pragma kernel backwards_pass_softmax_weights_biases_Adam

#pragma kernel forward_pass_linear
#pragma kernel backwards_pass_linear_inputs
#pragma kernel backwards_pass_linear_weights_biases_Adam

unsigned int input_column_size;
unsigned int input_row_size;
unsigned int weights_row_size;

StructuredBuffer<float> input;
RWStructuredBuffer<float> weights;
RWStructuredBuffer<float> biases;
RWStructuredBuffer<float> output;

//TODO: should see if a MATRIX_THREAD_COUNT_Y of 1 is faster, as inputs and outputs might just have the size of 1

float forward_pass(uint3 id, StructuredBuffer<float> input, RWStructuredBuffer<float> weights,  RWStructuredBuffer<float> biases);

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void forward_pass_ReLU(uint3 id : SV_DispatchThreadID)
{
    //TODO: checking for x value might not be necessary, out of bounds always returns zero
    if (id.x >= input_column_size || id.y >= weights_row_size)
        return;
    
    const float result = forward_pass(id, input, weights, biases);
    output[id.x * weights_row_size + id.y] = result < 0.0f ? 0.0f : result;
}

StructuredBuffer<float> d_values;
RWStructuredBuffer<float> d_inputs;

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_ReLU_inputs(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= input_row_size)
        return;

    const int d_column_index = id.x * weights_row_size;
    const int weight_column_index = id.y * weights_row_size;
    float d_input = 0;
    for (unsigned int i = 0; i < weights_row_size; ++i)
    {
        const int d_index = d_column_index + i;
        const float d_value = output[d_index] > 0.0f ? d_values[d_index] : 0.0f;

        d_input += d_value * weights[weight_column_index + i];
    }

    d_inputs[id.x * input_row_size + id.y] = d_input;
}

// Adam optimizer variables
float beta_1;
float beta_2;
float epsilon;
float current_learning_rate;
float reward_mean;
float beta_2_corrected;

RWStructuredBuffer<float> weights_momentum;
RWStructuredBuffer<float> weights_cache;

RWStructuredBuffer<float> biases_momentum;
RWStructuredBuffer<float> biases_cache;

void Adam_optimizer(const int index, const float d_value, RWStructuredBuffer<float> buffer_to_update,
                    RWStructuredBuffer<float> momentum, RWStructuredBuffer<float> cache);

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_ReLU_weights_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_row_size || id.y >= weights_row_size)
        return;

    float d_weight = 0;
    float d_bias = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        const int d_index = i * weights_row_size + id.y;
        const float d_value = output[d_index] > 0.0f ? d_values[d_index] : 0.0f;
        d_bias += d_value;

        // input matrix is accessed if it was transposed
        d_weight += input[i * input_row_size + id.x] * d_value;
    }

    const float weight_index = id.x * weights_row_size + id.y;
    Adam_optimizer(weight_index, d_weight, weights, weights_momentum, weights_cache);

    if (id.x != 0) return;

    Adam_optimizer(id.y, d_bias, biases, biases_momentum, biases_cache);
}

////////// Functions used for Tanh layers ///////////////

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void forward_pass_Tanh(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= weights_row_size)
        return;
    
    const float result = forward_pass(id, input, weights, biases);
    const float exp_pos = exp(result);
    const float exp_neg = exp(-result);

    output[id.x * weights_row_size + id.y] = (exp_pos - exp_neg) / (exp_pos + exp_neg);
}

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_Tanh_inputs(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= input_row_size)
        return;

    const int d_column_index = id.x * weights_row_size;
    const int weight_column_index = id.y * weights_row_size;
    float d_input = 0;
    for (unsigned int i = 0; i < weights_row_size; ++i)
    {
        const int d_index = d_column_index + i;
        const float output_value = output[d_index];
        const float d_value = (1 - output_value * output_value) * d_values[d_index];

        d_input += d_value * weights[weight_column_index + i];
    }

    d_inputs[id.x * input_row_size + id.y] = d_input;
}

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_Tanh_weights_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_row_size || id.y >= weights_row_size)
        return;

    float d_weight = 0;
    float d_bias = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        const int d_index = i * weights_row_size + id.y;
        const float output_value = output[d_index];
        const float d_value = (1 - output_value * output_value) * d_values[d_index];

        d_bias += d_value;
        d_weight += input[i * input_row_size + id.x] * d_value;
    }

    const float weight_index = id.x * weights_row_size + id.y;
    Adam_optimizer(weight_index, d_weight, weights, weights_momentum, weights_cache);

    if (id.x != 0) return;

    Adam_optimizer(id.y, d_bias, biases, biases_momentum, biases_cache);
}

////////// Functions used for Softmax layers ///////////////

unsigned int head_number;
unsigned int distribution_lenght;

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void forward_pass_softmax(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= head_number)
        return;

    const int current_distribution = distribution_lenght * id.y;
    const int start_index = id.x * weights_row_size + current_distribution;

    float sum = 0.0f;
    float max_value = -3.402823466e+38f;
    for (unsigned int i = 0; i < distribution_lenght; ++i)
    {
        max_value = max(max_value, output[start_index + i]);
    }

    for (unsigned int j = 0; j < distribution_lenght; ++j)
    {
        const int index = start_index + j;
        const float result = exp(output[index] - max_value);
        output[index] = result;
        sum += result;
    }
    for (unsigned int m = 0; m < distribution_lenght; ++m)
    {
        output[start_index + m] /= sum;
    }
}

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_softmax_inputs(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= input_row_size)
        return;

    const int d_column_index = id.x * weights_row_size;
    const int weight_column_index = id.y * weights_row_size;
    float d_input = 0;
    for (unsigned int i = 0; i < weights_row_size; ++i)
    {
        const int d_index = d_column_index + i;
        const float d_value = (output[d_index] - d_values[d_index]) / input_column_size;
        d_input += d_value * weights[weight_column_index + i];
    }

    d_inputs[id.x * input_row_size + id.y] = d_input;
}

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_softmax_weights_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_row_size || id.y >= weights_row_size)
        return;

    float d_weight = 0;
    float d_bias = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        const int d_index = i * weights_row_size + id.y;
        const float d_value = (output[d_index] - d_values[d_index]) / input_column_size;
        d_bias += d_value;
        d_weight += input[i * input_row_size + id.x] * d_value;
    }

    const float weight_index = id.x * weights_row_size + id.y;
    Adam_optimizer(weight_index, d_weight, weights, weights_momentum, weights_cache);

    if (id.x != 0) return;

    Adam_optimizer(id.y, d_bias, biases, biases_momentum, biases_cache);
}

////////// Functions used for linear layers ///////////////

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void forward_pass_linear(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= weights_row_size)
        return;
    
    output[id.x * weights_row_size + id.y] = forward_pass(id, input, weights, biases);
}

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_linear_inputs(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= input_row_size)
        return;

    const int d_column_index = id.x * weights_row_size;
    const int weight_column_index = id.y * weights_row_size;
    float d_input = 0;
    for (unsigned int i = 0; i < weights_row_size; ++i)
    {
        d_input += d_values[d_column_index + i] * weights[weight_column_index + i];
    }

    d_inputs[id.x * input_row_size + id.y] = d_input;
}

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_linear_weights_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_row_size || id.y >= weights_row_size)
        return;

    float d_weight = 0;
    float d_bias = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        const float d_value = d_values[i * weights_row_size + id.y];
        d_bias += d_value;
        d_weight += input[i * input_row_size + id.x] * d_value;
    }

    const float weight_index = id.x * weights_row_size + id.y;
    Adam_optimizer(weight_index, d_weight, weights, weights_momentum, weights_cache);

    if (id.x != 0) return;

    Adam_optimizer(id.y, d_bias, biases, biases_momentum, biases_cache);
}

inline float forward_pass(uint3 id, StructuredBuffer<float> input, RWStructuredBuffer<float> weights,  RWStructuredBuffer<float> biases)
{
    const int input_column_index = id.x * input_row_size;
    float result = 0;
    for (unsigned int i = 0; i < input_row_size; ++i)
    {
        result += input[input_column_index + i] * weights[i * weights_row_size + id.y];
    }
    return result + biases[id.y];
}

inline void Adam_optimizer(const int index, const float d_value, RWStructuredBuffer<float> buffer_to_update,
                           RWStructuredBuffer<float> momentum, RWStructuredBuffer<float> cache)
{
    momentum[index] = beta_1 * momentum[index] + (1 - beta_1) * d_value;
    const float momentum_corrected = momentum[index] / (1 - reward_mean);

    cache[index] = beta_2 * cache[index] + (1 - beta_2) * (d_value * d_value);
    const float cash_corrected = cache[index] / (1 - beta_2_corrected);

    buffer_to_update[index] += -current_learning_rate * momentum_corrected / (sqrt(cash_corrected) + epsilon);
}



////////// Loss Functions, currently they are not being used, might also be outdated and incomplete

// #pragma kernel forward_pass_MSE_loss
// #pragma kernel backwards_pass_MSE_loss
// #pragma kernel forward_pass_MSE_prioritized_loss
// #pragma kernel backwards_pass_MSE_prioritized_loss
// StructuredBuffer<float> y_true;
// RWStructuredBuffer<float> sample_losses;
// // number of threads can be different, this should be the best configuration for 1D arrays 
// [numthreads(32,1, 1)]
// void forward_pass_MSE_loss(uint3 id : SV_DispatchThreadID)
// {
//     if (id.x >= input_column_size)
//         return;
//
//     float result = 0;
//     for (unsigned int i = 0; i < weights_row_size; ++i)
//     {
//         const int index = id.x * weights_row_size + i;
//         const float error = y_true[index] - output[index];
//         result += error * error;
//     }
//
//     sample_losses[id.x] = result / weights_row_size;
// }
//
// StructuredBuffer<float> sample_weights;
//
// [numthreads(32,1, 1)]
// void forward_pass_MSE_prioritized_loss(uint3 id : SV_DispatchThreadID)
// {
//     if (id.x >= input_column_size)
//         return;
//
//     float result = 0;
//     for (unsigned int i = 0; i < weights_row_size; ++i)
//     {
//         const int index = id.x * weights_row_size + i;
//         const float error = y_true[index] - output[index];
//         //result += abs(error);
//         result += error * error;
//     }
//
//     //TODO: try it without j division;
//     sample_losses[id.x] = result / weights_row_size;
// }
//
// RWStructuredBuffer<float> d_inputs_loss;
//
// [numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
// void backwards_pass_MSE_loss(uint3 id : SV_DispatchThreadID)
// {
//     if (id.x >= input_column_size || id.y >= weights_row_size)
//         return;
//
//     const int index = id.x * weights_row_size + id.y;
//     d_inputs_loss[index] = -2.0f * (y_true[index] - output[index]) / weights_row_size / input_column_size;
// }
//
// [numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
// void backwards_pass_MSE_prioritized_loss(uint3 id : SV_DispatchThreadID)
// {
//     if (id.x >= input_column_size || id.y >= weights_row_size)
//         return;
//
//     const int index = id.x * weights_row_size + id.y;
//     d_inputs_loss[index] = -2.0f * (y_true[index] - output[index]) * sample_weights[id.x] / weights_row_size /
//         input_column_size;
// }



/*
[numthreads(32,1,1)]
void backwards_pass_ReLU_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= weights_row_size)
        return;

    float d_bias = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        const int d_index = i * weights_row_size + id.x;
        const float d_value = output[d_index] > 0.0f ? d_values[d_index] : 0.0f;
        d_bias += d_value;
    }

    Adam_optimizer(id.x, d_bias, biases, biases_momentum, biases_cache);
}

[numthreads(32,1,1)]
void backwards_pass_linear_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= weights_row_size)
        return;

    float d_bias = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        d_bias += d_values[i * weights_row_size + id.x];
    }

    Adam_optimizer(id.x, d_bias, biases, biases_momentum, biases_cache);
}
*/
