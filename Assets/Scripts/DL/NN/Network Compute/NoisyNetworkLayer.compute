#define MATRIX_THREAD_COUNT_X 8
#define MATRIX_THREAD_COUNT_Y 8

#pragma kernel forward_pass_ReLU
#pragma kernel backwards_pass_ReLU_inputs
#pragma kernel backwards_pass_ReLU_weights_biases_Adam
#pragma kernel backwards_pass_ReLU_biases_Adam

#pragma kernel forward_pass_Tanh
#pragma kernel backwards_pass_Tanh_inputs
#pragma kernel backwards_pass_Tanh_weights_biases_Adam
#pragma kernel backwards_pass_Tanh_biases_Adam

#pragma kernel forward_pass_linear
#pragma kernel backwards_pass_linear_inputs
#pragma kernel backwards_pass_linear_weights_biases_Adam
#pragma kernel backwards_pass_linear_biases_Adam

unsigned int input_column_size;
unsigned int input_row_size;
unsigned int weights_row_size;

StructuredBuffer<float> input;
RWStructuredBuffer<float> weights;
RWStructuredBuffer<float> biases;
RWStructuredBuffer<float> output;

RWStructuredBuffer<float> sigma_weights;
RWStructuredBuffer<float> sigma_biases;
StructuredBuffer<float> epsilon_inputs_outputs;

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void forward_pass_ReLU(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= weights_row_size)
        return;

    const float epsilon_output = epsilon_inputs_outputs[input_row_size + id.y];
    //const float factorized_epsilon_output = sign(epsilon_output) * sqrt(abs(epsilon_output));
    float result = 0;
    for (unsigned int i = 0; i < input_row_size; ++i)
    {
        const float epsilon_input = epsilon_inputs_outputs[i];
        const int weight_index = i * weights_row_size + id.y;

        result += input[id.x * input_row_size + i] * (weights[weight_index] + sigma_weights[weight_index] *
            epsilon_input * epsilon_output);
    }
    
    result += biases[id.y] + sigma_biases[id.y] * epsilon_output;

    output[id.x * weights_row_size + id.y] = result < 0.0f ? 0.0f : result;
}

StructuredBuffer<float> d_values;
RWStructuredBuffer<float> d_inputs;

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_ReLU_inputs(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= input_row_size)
        return;

    const float epsilon_input = epsilon_inputs_outputs[id.y];
    //const float factorized_epsilon_input = sign(epsilon_input) * sqrt(abs(epsilon_input));
    float d_input = 0;
    for (unsigned int i = 0; i < weights_row_size; ++i)
    {
        const int d_index = id.x * weights_row_size + i;
        const float d_value = output[d_index] > 0.0f ? d_values[d_index] : 0.0f;

        const float epsilon_output = epsilon_inputs_outputs[input_row_size + i];
        const int weight_index = id.y * weights_row_size + i;

        d_input += d_value * (weights[weight_index] + sigma_weights[weight_index] * epsilon_input * epsilon_output);
    }

    d_inputs[id.x * input_row_size + id.y] = d_input;
}

// Adam optimizer variables
float beta_1;
float beta_2;
float epsilon;
float current_learning_rate;
float beta_1_corrected;
float beta_2_corrected;

RWStructuredBuffer<float> weights_momentum;
RWStructuredBuffer<float> weights_cache;
RWStructuredBuffer<float> biases_momentum;
RWStructuredBuffer<float> biases_cache;

RWStructuredBuffer<float> sigma_weights_momentum;
RWStructuredBuffer<float> sigma_weights_cache;
RWStructuredBuffer<float> sigma_biases_momentum;
RWStructuredBuffer<float> sigma_biases_cache;

void Adam_optimizer(const int index, const float d_value, RWStructuredBuffer<float> buffer_to_update,
                    RWStructuredBuffer<float> momentum, RWStructuredBuffer<float> cache);

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_ReLU_weights_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_row_size || id.y >= weights_row_size)
        return;

    float d_weight = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        const int d_index = i * weights_row_size + id.y;
        const float d_value = output[d_index] > 0.0f ? d_values[d_index] : 0.0f;

        d_weight += input[i * input_row_size + id.x] * d_value;
    }

    const float weight_index = id.x * weights_row_size + id.y;
    Adam_optimizer(weight_index, d_weight, weights, weights_momentum, weights_cache);

    const float epsilon_input = epsilon_inputs_outputs[id.x];
    const float epsilon_output = epsilon_inputs_outputs[input_row_size + id.y];
    
    const float d_sigma_weight = d_weight * epsilon_input * epsilon_output;
    Adam_optimizer(weight_index, d_sigma_weight, sigma_weights, sigma_weights_momentum, sigma_weights_cache);
}

[numthreads(MATRIX_THREAD_COUNT_X,1,1)]
void backwards_pass_ReLU_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= weights_row_size)
        return;

    float d_bias = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        const int d_index = i * weights_row_size + id.y;
        d_bias += output[d_index] > 0.0f ? d_values[d_index] : 0.0f;
    }

    Adam_optimizer(id.x, d_bias, biases, biases_momentum, biases_cache);

    const float epsilon_output = epsilon_inputs_outputs[input_row_size + id.x];
    const float d_sigma_bias = d_bias * epsilon_output;
    Adam_optimizer(id.x, d_sigma_bias, sigma_biases, sigma_biases_momentum, sigma_biases_cache);
}

////////// Functions used for Tanh layers ///////////////

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void forward_pass_Tanh(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= weights_row_size)
        return;

    const float epsilon_output = epsilon_inputs_outputs[input_row_size + id.y];
    //const float factorized_epsilon_output = sign(epsilon_output) * sqrt(abs(epsilon_output));
    float result = 0;
    for (unsigned int i = 0; i < input_row_size; ++i)
    {
        const float epsilon_input = epsilon_inputs_outputs[i];
        const int weight_index = i * weights_row_size + id.y;

        result += input[id.x * input_row_size + i] * (weights[weight_index] + sigma_weights[weight_index] *
            epsilon_input * epsilon_output);
    }
    
    result += biases[id.y] + sigma_biases[id.y] * epsilon_output;

    const float exp_pos = exp(result);
    const float exp_neg = exp(-result);
    
    output[id.x * weights_row_size + id.y] = (exp_pos - exp_neg) / (exp_pos + exp_neg);
}

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_Tanh_inputs(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= input_row_size)
        return;

    const float epsilon_input = epsilon_inputs_outputs[id.y];
    //const float factorized_epsilon_input = sign(epsilon_input) * sqrt(abs(epsilon_input));
    float d_input = 0;
    for (unsigned int i = 0; i < weights_row_size; ++i)
    {
        const int d_index = id.x * weights_row_size + i;
        const float output_value =  output[d_index];
        const float d_value = (1 - output_value * output_value) * d_values[d_index];
        
        const float epsilon_output = epsilon_inputs_outputs[input_row_size + i];
        const int weight_index = id.y * weights_row_size + i;

        d_input += d_value * (weights[weight_index] + sigma_weights[weight_index] * epsilon_input * epsilon_output);
    }

    d_inputs[id.x * input_row_size + id.y] = d_input;
}

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_Tanh_weights_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_row_size || id.y >= weights_row_size)
        return;

    float d_weight = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        const int d_index = i * weights_row_size + id.y;
        const float output_value =  output[d_index];
        const float d_value = (1 - output_value * output_value) * d_values[d_index];

        d_weight += input[i * input_row_size + id.x] * d_value;
    }

    const float weight_index = id.x * weights_row_size + id.y;
    Adam_optimizer(weight_index, d_weight, weights, weights_momentum, weights_cache);

    const float epsilon_input = epsilon_inputs_outputs[id.x];
    const float epsilon_output = epsilon_inputs_outputs[input_row_size + id.y];
    
    const float d_sigma_weight = d_weight * epsilon_input * epsilon_output;
    Adam_optimizer(weight_index, d_sigma_weight, sigma_weights, sigma_weights_momentum, sigma_weights_cache);

}

[numthreads(MATRIX_THREAD_COUNT_X,1,1)]
void backwards_pass_Tanh_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= weights_row_size)
        return;

    float d_bias = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        const int d_index = i * weights_row_size + id.y;
        const float output_value =  output[d_index];

        d_bias += (1 - output_value * output_value) * d_values[d_index];
    }

    Adam_optimizer(id.x, d_bias, biases, biases_momentum, biases_cache);

    const float epsilon_output = epsilon_inputs_outputs[input_row_size + id.x];
    const float d_sigma_bias = d_bias * epsilon_output;
    Adam_optimizer(id.x, d_sigma_bias, sigma_biases, sigma_biases_momentum, sigma_biases_cache);
}

////////// Functions used for linear layers ///////////////

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void forward_pass_linear(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= weights_row_size)
        return;

    const float epsilon_output = epsilon_inputs_outputs[input_row_size + id.y];
    //const float factorized_epsilon_output = sign(epsilon_output) * sqrt(abs(epsilon_output));
    float result = 0;
    for (unsigned int i = 0; i < input_row_size; ++i)
    {
        const float epsilon_input = epsilon_inputs_outputs[i];
        const int weight_index = i * weights_row_size + id.y;
        result += input[id.x * input_row_size + i] * (weights[weight_index] + sigma_weights[weight_index] *
            epsilon_input * epsilon_output);
    }

    output[id.x * weights_row_size + id.y] = result + (biases[id.y] + sigma_biases[id.y] * epsilon_output);
}

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_linear_inputs(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_column_size || id.y >= input_row_size)
        return;

    const float epsilon_input = epsilon_inputs_outputs[id.y];
    //const float factorized_epsilon_input = sign(epsilon_input) * sqrt(abs(epsilon_input));
    float d_input = 0;
    for (unsigned int i = 0; i < weights_row_size; ++i)
    {
        const float epsilon_output = epsilon_inputs_outputs[input_row_size + i];
        const int weight_index = id.y * weights_row_size + i;

        d_input += d_values[id.x * weights_row_size + i] * (weights[weight_index] + sigma_weights[weight_index] *
            epsilon_input * epsilon_output);
    }

    d_inputs[id.x * input_row_size + id.y] = d_input;
}

[numthreads(MATRIX_THREAD_COUNT_X,MATRIX_THREAD_COUNT_Y,1)]
void backwards_pass_linear_weights_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= input_row_size || id.y >= weights_row_size)
        return;

    float d_weight = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        const float d_value = d_values[i * weights_row_size + id.y];
        d_weight += input[i * input_row_size + id.x] * d_value;
    }

    const float weight_index = id.x * weights_row_size + id.y;
    Adam_optimizer(weight_index, d_weight, weights, weights_momentum, weights_cache);

    const float epsilon_input = epsilon_inputs_outputs[id.x];
    const float epsilon_output = epsilon_inputs_outputs[input_row_size + id.y];
    const float d_sigma_weight = d_weight * epsilon_input * epsilon_output;
    
    Adam_optimizer(weight_index, d_sigma_weight, sigma_weights, sigma_weights_momentum, sigma_weights_cache);
}

[numthreads(MATRIX_THREAD_COUNT_X,1,1)]
void backwards_pass_linear_biases_Adam(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= weights_row_size)
        return;

    float d_bias = 0;
    for (unsigned int i = 0; i < input_column_size; ++i)
    {
        d_bias += d_values[i * weights_row_size + id.x];
    }

    Adam_optimizer(id.x, d_bias, biases, biases_momentum, biases_cache);

    const float epsilon_output = epsilon_inputs_outputs[input_row_size + id.x];
    const float d_sigma_bias = d_bias * epsilon_output;
    Adam_optimizer(id.x, d_sigma_bias, sigma_biases, sigma_biases_momentum, sigma_biases_cache);
}

inline void Adam_optimizer(const int index, const float d_value, RWStructuredBuffer<float> buffer_to_update,
                           RWStructuredBuffer<float> momentum, RWStructuredBuffer<float> cache)
{
    momentum[index] = beta_1 * momentum[index] + (1 - beta_1) * d_value;
    const float momentum_corrected = momentum[index] / (1 - beta_1_corrected);

    cache[index] = beta_2 * cache[index] + (1 - beta_2) * (d_value * d_value);
    const float cash_corrected = cache[index] / (1 - beta_2_corrected);

    buffer_to_update[index] += -current_learning_rate * momentum_corrected / (sqrt(cash_corrected) + epsilon);
}
